# Copyright (c) Kyutai, all rights reserved.
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

"""
Language Model Generation Module
================================

This module provides the LmGen class, which wraps the language model (Lm) to handle
the complexity of autoregressive generation with delayed audio streams.

=============================================================================
ROLE IN THE MOSHI SYSTEM
=============================================================================

LmGen is the RUNTIME ORCHESTRATOR for Moshi's generation process. While the Lm
class defines the model architecture, LmGen manages the complex bookkeeping
required for real-time, streaming generation with delayed audio streams.

In the Moshi pipeline:
1. User audio → Mimi encoder → audio tokens
2. LmGen receives user audio tokens and manages the generation loop
3. LmGen calls Lm._sample() to generate text + Moshi audio tokens
4. Generated audio tokens → Mimi decoder → Moshi speech

=============================================================================
THE DELAYED STREAMS CHALLENGE
=============================================================================

Moshi uses a "delayed streams" architecture where different audio codebooks
have different delays relative to the text stream:

    Text:     [t0] [t1] [t2] [t3] [t4] [t5] ...
    Audio 0:  [a0] [a1] [a2] [a3] [a4] [a5] ...  (delay=0, semantic)
    Audio 1:       [a0] [a1] [a2] [a3] [a4] ...  (delay=1 or 2, acoustic)
    Audio 2:            [a0] [a1] [a2] [a3] ...  (delay=1 or 2, acoustic)
    ...

This means:
- At step T, the model sees text token T-1 and audio tokens from T-1-delay
- Generated audio tokens are stored at their DELAYED positions
- Complete audio frames are only available after max_delay steps

LmGen handles all this complexity, providing a simple step() interface.

=============================================================================
SEQUENCE BUFFER STRUCTURE
=============================================================================

LmGen maintains a gen_sequence buffer with shape [B, num_codebooks, max_steps]:

    gen_sequence[:, 0, :]           = text tokens (Inner Monologue)
    gen_sequence[:, 1:1+dep_q, :]   = Moshi's audio tokens (generated)
    gen_sequence[:, 1+dep_q:, :]    = User's audio tokens (input)

Special token values:
- zero_token (-1): Produces zero embedding (no input)
- ungenerated_token (-2): Position not yet generated (validation)
- audio_padding_token: Padding for positions before delay window

=============================================================================
GENERATION FLOW (per step)
=============================================================================

1. RECEIVE USER AUDIO
   other_audio_tokens = mimi.encode_step(user_audio)
   
2. STORE USER AUDIO
   gen_sequence[:, 1+dep_q:, step] = other_audio_tokens
   
3. RETRIEVE DELAYED TOKENS
   For each codebook k with delay d:
       audio_input[k] = gen_sequence[:, k+1, step-1-d]
   
4. RUN MODEL
   text, audio = model._sample(prev_text, audio_input, ...)
   
5. STORE GENERATED TOKENS
   gen_sequence[:, 0, step] = text
   For each codebook k with delay d:
       gen_sequence[:, k+1, step-d] = audio[k]
   
6. RETURN COMPLETED AUDIO
   If step >= max_delay:
       return gen_sequence[:, 1:1+dep_q, step-max_delay]

=============================================================================
CLASSIFIER-FREE GUIDANCE (CFG)
=============================================================================

When cfg_coef != 1.0, LmGen enables classifier-free guidance:

    logits = cfg_coef * conditional - (cfg_coef - 1) * unconditional

This is passed through to Lm._sample() which handles the actual computation.
CFG improves generation quality by amplifying the conditional signal
relative to the unconditional baseline.

=============================================================================
KEY CONCEPTS
=============================================================================

- Delays: Each audio codebook has a delay (in steps) relative to the text stream.
  This means audio token at step T was generated based on context from step T-delay.
  
- Main Codebooks: The codebooks generated by the DepFormer (Moshi's audio output)
  Typically 8 or 16 codebooks for 8 or 16 RVQ levels.

- Other Codebooks: Additional codebooks from the input audio (user's speech)
  Also typically 8 or 16 codebooks, fed as input to condition Moshi's responses.

- CFG (Classifier-Free Guidance): A technique to improve generation quality by
  interpolating between conditional and unconditional predictions.

=============================================================================
USAGE EXAMPLE
=============================================================================

    # Initialize
    gen = LmGen(
        model=lm,
        max_steps=3000,  # ~4 minutes at 12.5 Hz
        text_sampler=Sampler(temp=0.7),
        audio_sampler=Sampler(temp=0.7),
    )
    
    # Generation loop
    for user_audio_chunk in audio_stream:
        # Encode user audio to tokens
        user_tokens = mimi.encode_step(user_audio_chunk)
        
        # Generate one step
        text_token = gen.step(user_tokens)
        
        # Get completed audio (accounts for delays)
        audio_tokens = gen.last_audio_tokens()
        if audio_tokens is not None:
            moshi_audio = mimi.decode_step(audio_tokens)
            play(moshi_audio)
"""

from typing import Optional

import mlx.core as mx

from ..models import Lm
from ..modules.conditioner import ConditionTensor
from ..utils import sampling


class LmGen:
    """
    Generation wrapper for the Moshi language model.
    
    ==========================================================================
    OVERVIEW
    ==========================================================================
    
    LmGen handles the autoregressive generation loop, managing the complex
    interplay between text and audio tokens with their respective delays.
    It maintains the full generation sequence and provides methods for
    stepping through generation one frame at a time.
    
    This class is the BRIDGE between the raw Lm model and real-time streaming
    applications. It handles:
    
    1. SEQUENCE MANAGEMENT
       - Maintains a buffer of all generated tokens
       - Tracks the current step in generation
       - Handles the delayed storage of audio tokens
    
    2. DELAY HANDLING
       - Retrieves tokens from correct delayed positions
       - Stores generated tokens at their delayed positions
       - Returns completed audio only when all codebooks are ready
    
    3. SAMPLING COORDINATION
       - Passes samplers to the model for text and audio
       - Supports classifier-free guidance
       - Provides hooks for custom processing
    
    ==========================================================================
    GENERATION PROCESS (per step)
    ==========================================================================
    
    1. Receive new audio tokens from the user (other_audio_tokens)
    2. Store user tokens in the sequence buffer
    3. Retrieve previously generated audio tokens (accounting for delays)
    4. Run the model to predict text and new audio tokens
    5. Store the generated tokens in the sequence buffer (at delayed positions)
    6. Return completed audio frame (if available after delay window)
    
    ==========================================================================
    ATTRIBUTES
    ==========================================================================
    
    Core State:
        batch_size: Number of parallel sequences to generate
        model: The underlying Lm model
        step_idx: Current step in the generation (0-indexed)
        gen_sequence: Buffer storing all tokens [B, num_codebooks, max_steps]
    
    Sampling:
        text_sampler: Sampler for text token distribution
        audio_sampler: Sampler for audio token distribution
        cfg_coef: Classifier-free guidance coefficient (1.0 = no guidance)
    
    Configuration:
        max_steps: Maximum number of generation steps
        num_codebooks: Total codebooks (1 text + audio_codebooks)
        main_codebooks: Number of codebooks generated by DepFormer
        audio_delays: List of delays for each audio codebook
        max_delay: Maximum delay across all codebooks
    
    Hooks:
        on_text_hook: Callback after text token generation
        on_audio_hook: Callback after audio token generation
    
    ==========================================================================
    EXAMPLE USAGE
    ==========================================================================
    
        >>> # Initialize generator
        >>> gen = LmGen(
        ...     model=lm,
        ...     max_steps=1000,
        ...     text_sampler=Sampler(temp=0.7),
        ...     audio_sampler=Sampler(temp=0.7),
        ... )
        >>> 
        >>> # Generation loop
        >>> for user_audio_frame in audio_stream:
        ...     # user_audio_frame: [B, other_codebooks] from Mimi encoder
        ...     text_token = gen.step(user_audio_frame)
        ...     
        ...     # Get completed audio (None until max_delay steps)
        ...     audio_output = gen.last_audio_tokens()
        ...     if audio_output is not None:
        ...         # audio_output: [B, main_codebooks]
        ...         decoded_audio = mimi.decode_step(audio_output)
    """
    
    def __init__(
        self,
        model: Lm,
        max_steps: int,
        text_sampler: sampling.Sampler,
        audio_sampler: sampling.Sampler,
        batch_size: int = 1,
        cfg_coef: float = 1.0,
        check: bool = False,
        on_text_hook=None,
        on_audio_hook=None,
    ):
        """
        Initialize the generation wrapper.
        
        Sets up the sequence buffer and configures generation parameters.
        The sequence buffer is initialized with "ungenerated" tokens to
        enable validation during generation.
        
        Args:
            model: The Lm model to use for generation. Must be fully initialized
                   with loaded weights.
            max_steps: Maximum number of steps to generate. At 12.5 Hz frame rate,
                      3000 steps ≈ 4 minutes of audio.
            text_sampler: Sampler configuration for text tokens. Controls temperature,
                         top-k, top-p for Inner Monologue generation.
            audio_sampler: Sampler configuration for audio tokens. Controls temperature,
                          top-k, top-p for audio codebook generation.
            batch_size: Number of parallel sequences (default: 1). All sequences
                       share the same model but have independent state.
            cfg_coef: Classifier-free guidance coefficient (default: 1.0).
                     1.0 = no guidance, >1.0 = stronger guidance.
                     Typical values: 1.0-3.0
            check: Whether to perform validation checks (default: False).
                  Enables additional assertions for debugging.
            on_text_hook: Optional callback called after text token generation.
                         Signature: fn(text_tokens: mx.array) -> None
                         Can modify text_tokens in-place for custom processing.
            on_audio_hook: Optional callback called after audio token generation.
                          Signature: fn(audio_tokens: mx.array) -> None
                          Can modify audio_tokens in-place for custom processing.
        
        Note:
            The hooks are particularly useful for:
            - TTS: Overriding text tokens with scripted input
            - Logging: Recording generation for debugging
            - Filtering: Applying custom constraints to generation
        """
        self.batch_size = batch_size
        self.model: Lm = model
        self.text_sampler = text_sampler
        self.audio_sampler = audio_sampler
        self.max_steps = max_steps
        self.check = check
        
        # =====================================================================
        # SEQUENCE BUFFER SETUP
        # =====================================================================
        # Total codebooks = 1 (text) + audio_codebooks
        # Layout: [text, moshi_audio_0, ..., moshi_audio_7, user_audio_0, ..., user_audio_7]
        self.num_codebooks = 1 + model.cfg.audio_codebooks
        
        # Initialize sequence buffer with "ungenerated" tokens
        # This enables validation: if we ever try to use an ungenerated token,
        # we know there's a bug in the delay handling logic
        self.gen_sequence = mx.full(
            shape=(self.batch_size, self.num_codebooks, max_steps),
            vals=self.ungenerated_token,
            dtype=mx.int32,
        )
        self.step_idx = 0
        
        # =====================================================================
        # AUDIO CONFIGURATION
        # =====================================================================
        # These come from the model config and define the delay structure
        self.audio_padding_token = self.model.cfg.audio_padding_token
        self.audio_delays = self.model.cfg.audio_delays  # e.g., [0,1,1,1,1,1,1,1] * 2 or [0,2,2,2,2,2,2,2] * 2
        self.max_delay = max(self.audio_delays)  # 1 for v0.1, 2 for newer models
        self.main_codebooks = self.model.cfg.depformer.num_slices  # Typically 8 or 16
        
        # =====================================================================
        # GENERATION SETTINGS
        # =====================================================================
        self.cfg_coef = cfg_coef
        self.on_text_hook = on_text_hook
        self.on_audio_hook = on_audio_hook

    @property
    def zero_token(self) -> int:
        """
        Special token value indicating no input should be provided.
        
        When this value appears in the input, the ScaledEmbedding layer
        produces a ZERO embedding vector (all zeros). This is used for:
        
        1. MASKING: Indicating "no audio" for positions where we don't
           want the model to receive any audio information.
        
        2. CFG: In classifier-free guidance, the unconditional path uses
           zero tokens to remove conditioning information.
        
        3. INITIALIZATION: Before the delay window starts, audio positions
           use zero tokens since no audio has been generated yet.
        
        The value -1 is chosen because:
        - It's clearly invalid as a vocabulary index
        - ScaledEmbedding explicitly checks for this value
        - It's easy to identify in debugging
        
        Returns:
            -1 (the zero token value)
        
        See Also:
            ScaledEmbedding.__call__() in lm.py for handling logic
        """
        return -1

    @property
    def ungenerated_token(self) -> int:
        """
        Special token value indicating a position that should be generated.
        
        This is used in the sequence buffer to mark positions that haven't
        been generated yet but SHOULD be predicted by the model. This enables:
        
        1. VALIDATION: If we ever try to use an ungenerated token as input,
           it indicates a bug in the delay handling logic.
        
        2. PARTIAL TEACHER FORCING: Some positions can be fixed (e.g., from
           a prefix) while others are marked for generation.
        
        3. DEBUGGING: Easy to identify positions that weren't properly filled.
        
        The value -2 is chosen to be distinct from:
        - Valid vocabulary indices (0 to vocab_size-1)
        - The zero token (-1)
        
        Returns:
            -2 (the ungenerated token value)
        
        Raises:
            ValueError: If this token is found in model input (indicates bug)
        """
        return -2

    def _step(
        self,
        other_audio_tokens: mx.array,
        ct: ConditionTensor | None = None,
        cross_attention_src: mx.array | None = None,
    ) -> tuple[mx.array, mx.array]:
        """
        Internal step function that performs one generation step.
        
        This is the CORE of the generation loop, handling all the complexity
        of delayed streams. It's called by step() and step_with_extra_heads().
        
        =======================================================================
        ALGORITHM
        =======================================================================
        
        1. PREPARE TEXT INPUT
           - Step 0: Use start token (vocab_size)
           - Step N: Use text token from step N-1
        
        2. STORE USER AUDIO
           - Store other_audio_tokens at current step position
           - These are the user's speech tokens from Mimi encoder
        
        3. RETRIEVE DELAYED AUDIO
           - For each codebook k with delay d:
             - If step-1-d >= 0: get token from gen_sequence
             - Else: use padding token (before delay window)
        
        4. RUN MODEL
           - Call model._sample() with text and audio inputs
           - Get predicted text token and audio tokens
        
        5. STORE GENERATED TOKENS
           - Store text at current step
           - Store audio at DELAYED positions (step - delay)
        
        =======================================================================
        DELAY HANDLING VISUALIZATION
        =======================================================================
        
        At step=3 with delays=[0, 1, 1, ...]:
        
        gen_sequence layout:
            step:     0    1    2    3    4    ...
            text:    [t0] [t1] [t2] [??] [  ]  <- store t3 at step 3
            audio0:  [a0] [a1] [a2] [??] [  ]  <- store a3 at step 3 (delay=0)
            audio1:  [  ] [a0] [a1] [a2] [??]  <- store a3 at step 2 (delay=1)
            ...
        
        Model input at step=3:
            text_input = t2 (from step 2)
            audio0_input = a1 (from step 3-1-0 = 2)
            audio1_input = a0 (from step 3-1-1 = 1)
        
        =======================================================================
        
        Args:
            other_audio_tokens: Audio tokens from the user's speech [B, other_codebooks]
                               These come from Mimi.encode_step() on user audio.
            ct: Optional condition tensor for guided generation (speaker, quality).
            cross_attention_src: Optional cross-attention source (e.g., for TTS
                                with speaker embeddings).
        
        Returns:
            Tuple of (text_tokens, transformer_output):
            - text_tokens: Generated text token [B, 1]
            - transformer_output: Hidden state from Temporal Transformer [B, 1, D]
        
        Raises:
            ValueError: If max_steps is exceeded or ungenerated tokens are found
                       in the input (indicates a bug in delay handling).
        """
        if self.step_idx >= self.max_steps:
            raise ValueError(f"reached max-steps {self.max_steps}")
        
        # =====================================================================
        # STEP 1: PREPARE TEXT INPUT
        # =====================================================================
        # The text input is the PREVIOUS step's text token (autoregressive)
        
        if self.step_idx == 0:
            # First step: use the special "start" token (vocab_size)
            # This signals the beginning of generation
            text_tokens = mx.full(
                shape=(self.batch_size, 1),
                vals=self.model.cfg.text_out_vocab_size,
                dtype=mx.int32,
            )
        else:
            # Subsequent steps: use the previously generated text token
            # gen_sequence[:, 0, :] contains text tokens
            text_tokens = self.gen_sequence[:, 0, self.step_idx - 1 : self.step_idx]
        
        # =====================================================================
        # STEP 2: STORE USER AUDIO TOKENS
        # =====================================================================
        # Store the "other" audio tokens (from user's speech) in the sequence
        # These are at indices [1 + main_codebooks : ] in the sequence buffer
        
        self.gen_sequence[:, 1 + self.main_codebooks :, self.step_idx] = (
            other_audio_tokens
        )
        
        # =====================================================================
        # STEP 3: RETRIEVE DELAYED AUDIO TOKENS FOR MODEL INPUT
        # =====================================================================
        # Each codebook has a delay, so we retrieve from different positions
        # This implements the delayed streams architecture
        
        audio_tokens = []
        for cb_idx, delay in enumerate(self.audio_delays):
            # Calculate which step to retrieve based on delay
            # We want the token from (current_step - 1 - delay)
            # -1 because we use the PREVIOUS step's token (autoregressive)
            # -delay because of the codebook's delay
            gen_idx = self.step_idx - 1 - delay
            
            if gen_idx >= 0:
                # Retrieve the token from the sequence buffer
                # cb_idx + 1 because index 0 is text
                audio_token = self.gen_sequence[:, cb_idx + 1, gen_idx][None]
            else:
                # Before the delay window: use padding token
                # This happens at the start of generation
                audio_token = mx.array([[self.audio_padding_token]])
            
            # Validate no ungenerated tokens in input
            # If we find one, it means our delay logic has a bug
            if (audio_token == self.ungenerated_token).any():  # type: ignore
                raise ValueError(
                    f"ungenerated value in audio tokens cb: {cb_idx} step: {self.step_idx}"
                )
            audio_tokens.append(audio_token)
        
        # Validate text tokens
        if (text_tokens == self.ungenerated_token).any():  # type: ignore
            raise ValueError(f"ungenerated value in text tokens {self.step_idx}")
        
        # =====================================================================
        # STEP 4: RUN MODEL INFERENCE
        # =====================================================================
        # Call the model's _sample method which:
        # 1. Sums embeddings (text + all audio)
        # 2. Runs Temporal Transformer
        # 3. Predicts text token
        # 4. Runs DepFormer to generate audio tokens
        
        text_tokens, audio_tokens, transformer_out = self.model._sample(
            text_tokens,
            audio_tokens,
            self.text_sampler,
            self.audio_sampler,
            ct=ct,
            cross_attention_src=cross_attention_src,
            cfg_coef=self.cfg_coef,
            on_text_hook=self.on_text_hook,
            on_audio_hook=self.on_audio_hook,
        )

        # Validate output shape
        assert audio_tokens is None or audio_tokens.shape[-2] == (
            self.model.cfg.generated_codebooks
        ), "invalid output audio-token shape"
        
        # =====================================================================
        # STEP 5: STORE GENERATED TOKENS
        # =====================================================================
        
        # Store text token at current step (no delay for text)
        self.gen_sequence[:, 0, self.step_idx] = text_tokens.squeeze(-1)
        
        # Store audio tokens at their DELAYED positions
        # This is the key insight: we store at (step - delay), not at step
        # This way, when we later retrieve with delay, we get the right token
        for cb_idx, delay in enumerate(self.audio_delays[: self.main_codebooks]):
            gen_idx = self.step_idx - delay
            if gen_idx >= 0:
                self.gen_sequence[:, cb_idx + 1, gen_idx] = audio_tokens[:, cb_idx, 0]
        
        self.step_idx += 1
        return text_tokens, transformer_out

    def step(
        self,
        other_audio_tokens: mx.array,
        ct: ConditionTensor | None = None,
        cross_attention_src: mx.array | None = None,
    ) -> mx.array:
        """
        Perform one generation step and return the text token.
        
        This is the MAIN INTERFACE for generation. Call this method with
        each frame of input audio to generate the corresponding output.
        
        =======================================================================
        TYPICAL USAGE IN REAL-TIME LOOP
        =======================================================================
        
            mimi.reset_all()
            gen = LmGen(model, max_steps=3000, ...)
            
            while streaming:
                # 1. Get user audio chunk (e.g., 80ms at 24kHz = 1920 samples)
                user_audio = microphone.read()
                
                # 2. Encode to tokens
                user_tokens = mimi.encode_step(user_audio)  # [1, 8]
                
                # 3. Generate one step
                text_token = gen.step(user_tokens)  # [1, 1]
                
                # 4. Get completed audio (may be None during warmup)
                audio_tokens = gen.last_audio_tokens()  # [1, 8] or None
                
                # 5. Decode and play
                if audio_tokens is not None:
                    moshi_audio = mimi.decode_step(audio_tokens)
                    speaker.write(moshi_audio)
        
        =======================================================================
        
        Args:
            other_audio_tokens: Audio tokens from user's speech [B, other_codebooks]
                               Shape is typically [1, 8] for single-batch with 8 codebooks.
                               These should come from Mimi.encode_step().
            ct: Optional condition tensor for guided generation.
                Used for speaker conditioning, quality control, etc.
            cross_attention_src: Optional cross-attention source.
                Used in TTS mode for speaker embedding conditioning.
        
        Returns:
            Generated text tokens [B, 1]
            This is the Inner Monologue text prediction for this timestep.
        
        Note:
            To get the generated audio, call last_audio_tokens() after step().
            Audio is returned separately because of the delay structure.
        """
        return self._step(other_audio_tokens, ct, cross_attention_src)

    def step_with_extra_heads(
        self,
        other_audio_tokens: mx.array,
        ct: ConditionTensor | None = None,
        cross_attention_src: mx.array | None = None,
    ) -> tuple[mx.array, list[mx.array]]:
        """
        Perform one generation step and return text token plus extra head outputs.
        
        Some Moshi models have additional prediction heads for auxiliary tasks:
        - Voice Activity Detection (VAD)
        - Emotion classification
        - Speaker identification
        - Turn-taking prediction
        
        This method returns both the main text prediction and the probability
        distributions from all extra heads, enabling multi-task inference.
        
        Args:
            other_audio_tokens: Audio tokens from user's speech [B, other_codebooks]
            ct: Optional condition tensor for guided generation
            cross_attention_src: Optional cross-attention source
        
        Returns:
            Tuple of (text_tokens, extra_head_probs):
            - text_tokens: Generated text token [B, 1]
            - extra_head_probs: List of probability distributions [B, 1, head_dim]
                               One entry per extra head in the model.
        
        Example:
            >>> text, extra = gen.step_with_extra_heads(user_tokens)
            >>> vad_probs = extra[0]  # Voice activity probabilities
            >>> is_speaking = vad_probs[0, 0, 1] > 0.5  # Check if user speaking
        """
        text, transformer_out = self._step(other_audio_tokens, ct, cross_attention_src)
        
        # Compute softmax probabilities for each extra head
        # Each head projects transformer_out to its own output dimension
        extra_heads = [
            mx.softmax(eh(transformer_out), axis=-1) for eh in self.model.extra_heads
        ]
        return text, extra_heads

    def last_audio_tokens(self) -> Optional[mx.array]:
        """
        Retrieve the most recently completed audio tokens.
        
        Due to the delay structure, audio tokens are only "complete" after
        ALL codebooks have been generated for that position. This method
        returns the audio tokens from max_delay steps ago, which is the
        most recent position where all codebooks are available.
        
        =======================================================================
        WHY DELAYS MATTER
        =======================================================================
        
        With delays = [0, 1, 1, 1, 1, 1, 1, 1]:
        - Codebook 0 (semantic): available immediately
        - Codebooks 1-7 (acoustic): available 1 step later
        
        At step 5:
        - Position 5: only codebook 0 is filled
        - Position 4: all codebooks are filled ← this is what we return
        
        So we return position (step - 1 - max_delay) = (5 - 1 - 1) = 3
        
        =======================================================================
        WARMUP PERIOD
        =======================================================================
        
        During the first max_delay steps, this method returns None because
        no complete audio frames are available yet. This is the "warmup"
        period where the model is building up its delayed state.
        
        =======================================================================
        
        Returns:
            Audio tokens [B, main_codebooks] if available, None otherwise.
            
            Returns None if:
            - Not enough steps have passed (still in warmup period)
            - Any token is still the padding token (incomplete frame)
        
        Raises:
            ValueError: If ungenerated tokens are found (indicates a bug
                       in the delay handling logic).
        
        Example:
            >>> for i in range(10):
            ...     gen.step(user_tokens)
            ...     audio = gen.last_audio_tokens()
            ...     if audio is not None:
            ...         # audio is [B, 8] - all 8 codebooks for one frame
            ...         decoded = mimi.decode_step(audio)
        """
        # Calculate the index of the most recent complete audio frame
        # We need to go back max_delay steps to ensure all codebooks are filled
        gen_idx = self.step_idx - 1 - self.max_delay
        if gen_idx < 0:
            # Still in warmup period
            return None
        
        # Extract tokens for all main codebooks at this position
        # Indices 1 to 1+main_codebooks are Moshi's generated audio
        tokens = self.gen_sequence[:, 1 : 1 + self.main_codebooks, gen_idx]

        # Check if any tokens are still padding (not yet generated)
        # This can happen if delays are non-uniform
        if (tokens == self.audio_padding_token).any():  # type: ignore
            return None
        
        # Validate no ungenerated tokens (would indicate a bug)
        if (tokens == self.ungenerated_token).any():  # type: ignore
            raise ValueError(f"ungenerated value in last-audio tokens {self.step_idx}")
        
        return tokens
