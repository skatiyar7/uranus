# Copyright (c) Kyutai, all rights reserved.
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

"""
Language Model Generation Module
================================

This module provides the LmGen class, which wraps the language model (Lm) to handle
the complexity of autoregressive generation with delayed audio streams.

The Moshi model uses a "delayed streams" architecture where different audio codebooks
have different delays relative to the text stream. This allows the model to:
1. Generate text tokens based on recent audio context
2. Generate audio tokens that are temporally aligned with the text

The LmGen class manages:
- Token sequence storage with proper delay handling
- Sampling from text and audio distributions
- Coordination between the main transformer and depformer
- Classifier-free guidance (CFG) for improved generation quality

Key Concepts:
-------------
- Delays: Each audio codebook has a delay (in steps) relative to the text stream.
  This means audio token at step T was generated based on context from step T-delay.
  
- Main Codebooks: The codebooks generated by the depformer (model's audio output)

- Other Codebooks: Additional codebooks from the input audio (user's speech)

- CFG (Classifier-Free Guidance): A technique to improve generation quality by
  interpolating between conditional and unconditional predictions.
"""

from typing import Optional

import mlx.core as mx

from ..models import Lm
from ..modules.conditioner import ConditionTensor
from ..utils import sampling


class LmGen:
    """
    Generation wrapper for the Moshi language model.
    
    LmGen handles the autoregressive generation loop, managing the complex
    interplay between text and audio tokens with their respective delays.
    It maintains the full generation sequence and provides methods for
    stepping through generation one frame at a time.
    
    The generation process at each step:
    1. Receive new audio tokens from the user (other_audio_tokens)
    2. Retrieve previously generated audio tokens (accounting for delays)
    3. Run the model to predict text and new audio tokens
    4. Store the generated tokens in the sequence buffer
    
    Attributes:
        batch_size: Number of parallel sequences to generate
        model: The underlying Lm model
        text_sampler: Sampler for text token distribution
        audio_sampler: Sampler for audio token distribution
        max_steps: Maximum number of generation steps
        gen_sequence: Buffer storing all generated tokens [B, num_codebooks, max_steps]
        step_idx: Current step in the generation
        cfg_coef: Classifier-free guidance coefficient (1.0 = no guidance)
    
    Example:
        >>> gen = LmGen(model, max_steps=1000, text_sampler=Sampler(), audio_sampler=Sampler())
        >>> for audio_frame in audio_stream:
        ...     text_token = gen.step(audio_frame)
        ...     audio_output = gen.last_audio_tokens()
    """
    
    def __init__(
        self,
        model: Lm,
        max_steps: int,
        text_sampler: sampling.Sampler,
        audio_sampler: sampling.Sampler,
        batch_size: int = 1,
        cfg_coef: float = 1.0,
        check: bool = False,
        on_text_hook=None,
        on_audio_hook=None,
    ):
        """
        Initialize the generation wrapper.
        
        Args:
            model: The Lm model to use for generation
            max_steps: Maximum number of steps to generate
            text_sampler: Sampler configuration for text tokens
            audio_sampler: Sampler configuration for audio tokens
            batch_size: Number of parallel sequences (default: 1)
            cfg_coef: Classifier-free guidance coefficient (default: 1.0)
            check: Whether to perform validation checks (default: False)
            on_text_hook: Optional callback called after text token generation
            on_audio_hook: Optional callback called after audio token generation
        """
        self.batch_size = batch_size
        self.model: Lm = model
        self.text_sampler = text_sampler
        self.audio_sampler = audio_sampler
        self.max_steps = max_steps
        self.check = check
        
        # Total codebooks = 1 (text) + audio_codebooks
        self.num_codebooks = 1 + model.cfg.audio_codebooks
        
        # Initialize sequence buffer with "ungenerated" tokens
        self.gen_sequence = mx.full(
            shape=(self.batch_size, self.num_codebooks, max_steps),
            vals=self.ungenerated_token,
            dtype=mx.int32,
        )
        self.step_idx = 0
        
        # Audio configuration from model
        self.audio_padding_token = self.model.cfg.audio_padding_token
        self.audio_delays = self.model.cfg.audio_delays
        self.max_delay = max(self.audio_delays)
        self.main_codebooks = self.model.cfg.depformer.num_slices
        
        # Generation settings
        self.cfg_coef = cfg_coef
        self.on_text_hook = on_text_hook
        self.on_audio_hook = on_audio_hook

    @property
    def zero_token(self) -> int:
        """
        Special token value indicating no input should be provided.
        
        When this value appears in the input, the model should not receive
        any embedding for that position (effectively zero embedding).
        
        Returns:
            -1 (the zero token value)
        """
        return -1

    @property
    def ungenerated_token(self) -> int:
        """
        Special token value indicating a position that should be generated.
        
        This is used in the sequence buffer to mark positions that haven't
        been generated yet but should be predicted by the model. This enables
        partial teacher forcing where some modalities are fixed while others
        are generated.
        
        Returns:
            -2 (the ungenerated token value)
        """
        return -2

    def _step(
        self,
        other_audio_tokens: mx.array,
        ct: ConditionTensor | None = None,
        cross_attention_src: mx.array | None = None,
    ) -> tuple[mx.array, mx.array]:
        """
        Internal step function that performs one generation step.
        
        This method:
        1. Prepares the text input (previous text token or initial token)
        2. Stores the incoming audio tokens in the sequence buffer
        3. Retrieves delayed audio tokens for model input
        4. Runs the model to generate new text and audio tokens
        5. Stores the generated tokens in the sequence buffer
        
        Args:
            other_audio_tokens: Audio tokens from the user's speech [B, other_codebooks]
            ct: Optional condition tensor for guided generation
            cross_attention_src: Optional cross-attention source (e.g., for TTS)
        
        Returns:
            Tuple of (text_tokens, transformer_output)
        
        Raises:
            ValueError: If max_steps is exceeded or ungenerated tokens are found
        """
        if self.step_idx >= self.max_steps:
            raise ValueError(f"reached max-steps {self.max_steps}")
        
        # =====================================================================
        # Prepare Text Input
        # =====================================================================
        
        if self.step_idx == 0:
            # First step: use the special "start" token (vocab_size)
            text_tokens = mx.full(
                shape=(self.batch_size, 1),
                vals=self.model.cfg.text_out_vocab_size,
                dtype=mx.int32,
            )
        else:
            # Subsequent steps: use the previously generated text token
            text_tokens = self.gen_sequence[:, 0, self.step_idx - 1 : self.step_idx]
        
        # =====================================================================
        # Store Input Audio Tokens
        # =====================================================================
        
        # Store the "other" audio tokens (from user's speech) in the sequence
        self.gen_sequence[:, 1 + self.main_codebooks :, self.step_idx] = (
            other_audio_tokens
        )
        
        # =====================================================================
        # Retrieve Delayed Audio Tokens for Model Input
        # =====================================================================
        
        audio_tokens = []
        for cb_idx, delay in enumerate(self.audio_delays):
            # Calculate which step to retrieve based on delay
            gen_idx = self.step_idx - 1 - delay
            if gen_idx >= 0:
                # Retrieve the token from the sequence buffer
                audio_token = self.gen_sequence[:, cb_idx + 1, gen_idx][None]
            else:
                # Before the delay window: use padding token
                audio_token = mx.array([[self.audio_padding_token]])
            
            # Validate no ungenerated tokens in input
            if (audio_token == self.ungenerated_token).any():  # type: ignore
                raise ValueError(
                    f"ungenerated value in audio tokens cb: {cb_idx} step: {self.step_idx}"
                )
            audio_tokens.append(audio_token)
        
        # Validate text tokens
        if (text_tokens == self.ungenerated_token).any():  # type: ignore
            raise ValueError(f"ungenerated value in text tokens {self.step_idx}")
        
        # =====================================================================
        # Run Model Inference
        # =====================================================================
        
        text_tokens, audio_tokens, transformer_out = self.model._sample(
            text_tokens,
            audio_tokens,
            self.text_sampler,
            self.audio_sampler,
            ct=ct,
            cross_attention_src=cross_attention_src,
            cfg_coef=self.cfg_coef,
            on_text_hook=self.on_text_hook,
            on_audio_hook=self.on_audio_hook,
        )

        # Validate output shape
        assert audio_tokens is None or audio_tokens.shape[-2] == (
            self.model.cfg.generated_codebooks
        ), "invalid output audio-token shape"
        
        # =====================================================================
        # Store Generated Tokens
        # =====================================================================
        
        # Store text token at current step
        self.gen_sequence[:, 0, self.step_idx] = text_tokens.squeeze(-1)
        
        # Store audio tokens at their delayed positions
        for cb_idx, delay in enumerate(self.audio_delays[: self.main_codebooks]):
            gen_idx = self.step_idx - delay
            if gen_idx >= 0:
                self.gen_sequence[:, cb_idx + 1, gen_idx] = audio_tokens[:, cb_idx, 0]
        
        self.step_idx += 1
        return text_tokens, transformer_out

    def step(
        self,
        other_audio_tokens: mx.array,
        ct: ConditionTensor | None = None,
        cross_attention_src: mx.array | None = None,
    ) -> mx.array:
        """
        Perform one generation step and return the text token.
        
        This is the main interface for generation. Call this method with
        each frame of input audio to generate the corresponding text output.
        
        Args:
            other_audio_tokens: Audio tokens from user's speech [B, other_codebooks]
            ct: Optional condition tensor for guided generation
            cross_attention_src: Optional cross-attention source
        
        Returns:
            Generated text tokens [B, 1]
        """
        return self._step(other_audio_tokens, ct, cross_attention_src)

    def step_with_extra_heads(
        self,
        other_audio_tokens: mx.array,
        ct: ConditionTensor | None = None,
        cross_attention_src: mx.array | None = None,
    ) -> tuple[mx.array, list[mx.array]]:
        """
        Perform one generation step and return text token plus extra head outputs.
        
        Some models have additional prediction heads (e.g., for auxiliary tasks).
        This method returns both the main text prediction and the softmax
        probabilities from all extra heads.
        
        Args:
            other_audio_tokens: Audio tokens from user's speech
            ct: Optional condition tensor
            cross_attention_src: Optional cross-attention source
        
        Returns:
            Tuple of (text_tokens, list of extra head probability distributions)
        """
        text, transformer_out = self._step(other_audio_tokens, ct, cross_attention_src)
        
        # Compute softmax probabilities for each extra head
        extra_heads = [
            mx.softmax(eh(transformer_out), axis=-1) for eh in self.model.extra_heads
        ]
        return text, extra_heads

    def last_audio_tokens(self) -> Optional[mx.array]:
        """
        Retrieve the most recently completed audio tokens.
        
        Due to the delay structure, audio tokens are only "complete" after
        all codebooks have been generated for that position. This method
        returns the audio tokens from max_delay steps ago, which is the
        most recent position where all codebooks are available.
        
        Returns:
            Audio tokens [B, main_codebooks] if available, None otherwise.
            Returns None if:
            - Not enough steps have passed (still in warmup)
            - Any token is still the padding token
        
        Raises:
            ValueError: If ungenerated tokens are found (indicates a bug)
        """
        # Calculate the index of the most recent complete audio frame
        gen_idx = self.step_idx - 1 - self.max_delay
        if gen_idx < 0:
            return None
        
        # Extract tokens for all main codebooks at this position
        tokens = self.gen_sequence[:, 1 : 1 + self.main_codebooks, gen_idx]

        # Check if any tokens are still padding (not yet generated)
        if (tokens == self.audio_padding_token).any():  # type: ignore
            return None
        
        # Validate no ungenerated tokens (would indicate a bug)
        if (tokens == self.ungenerated_token).any():  # type: ignore
            raise ValueError(f"ungenerated value in last-audio tokens {self.step_idx}")
        
        return tokens
